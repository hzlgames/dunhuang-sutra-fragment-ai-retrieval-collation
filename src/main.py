import argparse
import json
import os
from datetime import datetime
from pathlib import Path
from typing import Iterable, List

from dotenv import load_dotenv
load_dotenv()

from src.ai_agent import CBETAAgent, AgentConfig, StreamHandler
from src.schemas import FinalAnswer
from src.config import get_output_dir


def parse_args():
    parser = argparse.ArgumentParser(
        description="åˆ©ç”¨ Gemini å®˜æ–¹ API + CBETA å·¥å…·å®Œæˆ OCR â†’ æ¨ç† â†’ è€ƒè¯çš„å…¨æµç¨‹è°ƒè¯•ã€‚"
    )
    parser.add_argument(
        "--input",
        default="input",
        help="å¾…å¤„ç†å›¾ç‰‡ç›®å½•ï¼Œé»˜è®¤ input/",
    )
    parser.add_argument(
        "--output",
        default=None,
        help="ç»“æœä¸æ—¥å¿—è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨ç¯å¢ƒå˜é‡ OUTPUT_DIR æˆ– output/",
    )
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="å…³é—­æ§åˆ¶å°æµå¼è¾“å‡ºï¼Œä»…å†™å…¥æ—¥å¿—æ–‡ä»¶ã€‚",
    )
    return parser.parse_args()


def iter_images(input_dir: Path) -> Iterable[Path]:
    supported = {".png", ".jpg", ".jpeg"}
    for path in sorted(input_dir.iterdir()):
        if path.suffix.lower() in supported:
            yield path


def build_stream_handler(log_path: Path, mirror_stdout: bool) -> StreamHandler:
    log_path.parent.mkdir(parents=True, exist_ok=True)
    log_file = log_path.open("w", encoding="utf-8")

    def handler(event_type: str, payload):
        entry = {
            "ts": datetime.now().isoformat(),
            "event": event_type,
            "payload": payload,
        }
        log_file.write(json.dumps(entry, ensure_ascii=False) + "\n")
        log_file.flush()

        if not mirror_stdout:
            return

        text = (payload or {}).get("text", "")
        if event_type == "thought":
            print(f"ğŸ§  {text.strip()}")
        elif event_type == "text":
            print(f"ğŸ’¬ {text.strip()}")
        elif event_type == "tool_call":
            print(f"ğŸ› ï¸  è°ƒç”¨ {payload.get('name')}ï¼Œå‚æ•°: {payload.get('args')}")
        elif event_type == "tool_result":
            summary = payload.get("summary", "")
            print(f"âœ… å·¥å…· {payload.get('name')} å®Œæˆ: {summary}")
        elif event_type == "error":
            print(f"âš ï¸ {payload.get('message')}")

    def close():
        log_file.close()

    handler.close = close  # type: ignore[attr-defined]
    return handler


def summarize_final_answer(answer: FinalAnswer) -> str:
    lines: List[str] = []

    lines.append("========== OCR æ‘˜è¦ ==========")
    lines.append(answer.ocr_result.recognized_text.strip() or "(ç©º)")
    if answer.ocr_notes:
        lines.append("\né€åˆ—/é€å¥è¯´æ˜ï¼š")
        for idx, note in enumerate(answer.ocr_notes, 1):
            lines.append(f"  {idx}. {note}")
    if answer.ocr_result.uncertain_chars:
        lines.append(f"\nä¸ç¡®å®šå­—ç¬¦ï¼š{', '.join(answer.ocr_result.uncertain_chars)}")
    if answer.ocr_result.word_segmentation:
        lines.append(f"åˆ†è¯ç‰‡æ®µï¼š{', '.join(answer.ocr_result.word_segmentation)}")

    # ç‰‡æ®µå…³é”®ä¿¡æ¯ï¼šç‰©è´¨å½¢æ€ã€é¢˜è®°ã€ç‰ˆå¼ç­‰
    if getattr(answer, "key_facts", None):
        lines.append("\n========== ç‰‡æ®µå…³é”®ä¿¡æ¯ ==========")
        for idx, fact in enumerate(answer.key_facts, 1):
            lines.append(f"{idx}. {fact}")

    lines.append("\n========== å€™é€‰ç»æ–‡ï¼ˆæŒ‰ç½®ä¿¡åº¦ï¼‰ ==========")
    if answer.scripture_locations:
        for idx, loc in enumerate(answer.scripture_locations, 1):
            lines.append(f"{idx}. {loc.work_title} ({loc.work_id})")
            lines.append(
                f"   å·: {loc.juan} | è—ç»: {loc.canon or 'æœªçŸ¥'} | æœä»£: {loc.dynasty or 'æœªçŸ¥'} | ä½œè¯‘è€…: {loc.author or 'æœªçŸ¥'}"
            )
            lines.append(
                f"   ç½®ä¿¡åº¦: {loc.confidence:.2f} | ä¾æ®: {loc.confidence_reason}"
            )
            lines.append(f"   åŒ¹é…ç‰‡æ®µ: {loc.snippet}")

            # è‹¥æ¨¡å‹å·²ç»™å‡ºå¤–éƒ¨åœ¨çº¿é˜…è§ˆé“¾æ¥ï¼ˆå¦‚ Gallicaï¼‰ï¼Œç›´æ¥å±•ç¤º
            if getattr(loc, "external_url", None):
                source_label = getattr(loc, "source", None) or "å¤–éƒ¨"
                lines.append(f"   {source_label}åœ¨çº¿é˜…è§ˆ: {loc.external_url}")
            else:
                # é»˜è®¤å‡å®šä¸º CBETA ç»æ–‡ï¼Œé™„å¸¦å¯ç›´æ¥æ‰“å¼€çš„ CBETA åœ¨çº¿é˜…è§ˆé“¾æ¥
                # çº¦å®šï¼šwork_id å¦‚ T0001ï¼Œå·å· loc.juan å¯è½¬ä¸ºä¸‰ä½æ•°å­—ï¼Œä¾‹å¦‚ 1 -> 001
                cbeta_url = None
                try:
                    juan_num = int(str(loc.juan).strip())
                    cbeta_url = f"https://cbetaonline.dila.edu.tw/zh/{loc.work_id}_{juan_num:03d}"
                except (ValueError, TypeError):
                    # å·å·æ— æ³•è½¬ä¸ºæ•´æ•°æ—¶ï¼Œåªç»™ä¸€ä¸ªæŒ‰ç»å·æœç´¢çš„å¤‡ç”¨é“¾æ¥
                    cbeta_url = f"https://cbetaonline.dila.edu.tw/zh/search?keyword={loc.work_id}"

                lines.append(f"   åœ¨çº¿é˜…è§ˆ: {cbeta_url}")
    else:
        lines.append("æš‚æ— å¯ä¿¡å€™é€‰ï¼Œè¯·æ‰‹åŠ¨ç»§ç»­æœç´¢ã€‚")

    if answer.candidate_insights:
        lines.append("\nè¡¥å……æ´å¯Ÿï¼š")
        for idx, insight in enumerate(answer.candidate_insights, 1):
            lines.append(f"  - {insight}")

    lines.append("\n========== æ ¡å¯¹æç¤º ==========")
    if answer.verification_points:
        for idx, point in enumerate(answer.verification_points, 1):
            lines.append(f"{idx}. {point}")
    else:
        lines.append("æš‚æ— ç‰¹åˆ«æç¤ºï¼Œå¯ä¾æ®ä¸Šæ–¹å€™é€‰ç»§ç»­äººå·¥æ ¸å¯¹ã€‚")

    lines.append("\n========== å»ºè®®çš„ä¸‹ä¸€æ­¥ ==========")
    if answer.next_actions:
        for idx, action in enumerate(answer.next_actions, 1):
            lines.append(f"{idx}. {action}")
    else:
        lines.append("æœªæä¾›å…·ä½“å»ºè®®ã€‚")

    lines.append("\n========== æ¨ç†ä¸å·¥å…· ==========")
    lines.append(answer.reasoning.strip() or "(æ— )")
    lines.append(f"\næœç´¢è¿­ä»£æ¬¡æ•°: {answer.search_iterations}")
    lines.append(f"ä½¿ç”¨å·¥å…·: {', '.join(answer.tools_used) if answer.tools_used else 'æ— '}")
    lines.append(f"ä¼šè¯ ID: {answer.session_id or 'N/A'}")

    return "\n".join(lines)


def build_fragment_note(answer: FinalAnswer, image_name: str) -> str:
    """
    æ„é€ å•å¼ å›¾ç‰‡å¯¹åº”çš„â€œæ–‡çŒ®æ•´ç†è¯´æ˜â€ï¼Œé£æ ¼å‚è€ƒ `æ–‡çŒ®æ•´ç†ç»“æœç¤ºä¾‹.txt`ã€‚
    - image_nameï¼šä¸å«æ‰©å±•åçš„æ–‡ä»¶åï¼Œç›´æ¥ä½œä¸ºç¼–å·ä½¿ç”¨ï¼ˆå¦‚ï¼šP.3801ã€Ğ”Ñ….00931ï¼‰ã€‚
    """
    lines: List[str] = []

    # ç¼–å·è¡Œ
    lines.append(image_name)

    # æ–‡çŒ®å†…å®¹
    lines.append("æ–‡çŒ®å†…å®¹ï¼š")
    if answer.scripture_locations:
        for idx, loc in enumerate(answer.scripture_locations, 1):
            # åŸºæœ¬ä¹¦ç›®ä¿¡æ¯ï¼ˆCBETA / Gallica å…±ç”¨éª¨æ¶ï¼‰
            base_parts: List[str] = []
            # (1) ç»å
            base_parts.append(loc.work_title)
            # (2) ç»å·ï¼ˆå¦‚ CBETA T08, no. 235ï¼‰
            if loc.canon and loc.work_id:
                # work_id é€šå¸¸ä¸º T0235/X0021 ç­‰ï¼Œè¿™é‡Œæ‹†æˆè—ç»/ç¼–å·ä¸¤éƒ¨åˆ†
                canon_code = loc.canon
                work_code = loc.work_id
                base_parts.append(f"CBETAï¼Œ{canon_code}ï¼Œno.{work_code.lstrip(canon_code)}")
            elif loc.work_id:
                base_parts.append(f"ç¼–å·ï¼š{loc.work_id}")
            # (3) è¯‘è€…/ä½œè€…
            if loc.author:
                base_parts.append(f"{loc.author}è¯‘")

            # ç»„åˆä¸»å¥
            main_sentence = "ï¼Œ".join(base_parts) if base_parts else loc.work_title

            # å®Œæ•´åº¦æè¿°ç”± AI åœ¨ç½®ä¿¡åº¦ç†ç”±ä¸­å¸¸ä¼šä½“ç°ï¼Œè¿™é‡Œç®€åŒ–å¼•ç”¨
            completeness = ""
            if "é¦–å°¾" in (loc.confidence_reason or ""):
                completeness = loc.confidence_reason

            # Gallica / å…¶ä»–æ¥æºæ ‡è®°
            source_label = getattr(loc, "source", None)
            if source_label and source_label.lower() == "gallica":
                main_sentence += "ï¼ˆGallica å†™æœ¬ï¼‰"

            # è¾“å‡ºä¸€æ¡æ–‡çŒ®å†…å®¹è¯´æ˜
            content_line = f"ï¼ˆ{idx}ï¼‰{main_sentence}"
            if completeness:
                content_line += f"ã€‚{completeness}"
            lines.append(content_line)

            # è‹¥æœ‰åœ¨çº¿é“¾æ¥ï¼Œç»§ç»­åœ¨å†…å®¹éƒ¨åˆ†ç»™å‡º
            external_url = getattr(loc, "external_url", None)
            if external_url:
                if source_label and source_label.lower() == "gallica":
                    lines.append(f"    Gallica åœ¨çº¿é˜…è§ˆï¼š{external_url}")
                else:
                    lines.append(f"    åœ¨çº¿é˜…è§ˆï¼š{external_url}")
    else:
        lines.append("ï¼ˆæš‚æœªèƒ½æ˜ç¡®å®šä½å¯¹åº”ç»æ–‡ï¼Œéœ€äººå·¥è¡¥å……ã€‚ï¼‰")

    # ç‰©è´¨å½¢æ€ï¼š
    if answer.key_facts:
        for idx, fact in enumerate(answer.key_facts, 1):
            lines.append(f"{idx}. {fact}")
    else:
        lines.append("ç‰©è´¨å½¢æ€ï¼šï¼ˆæœ¬å·¥å…·æš‚æ— æ³•ä»å›¾åƒä¸­ç²¾ç¡®åˆ¤æ–­è£…å¸§ä¸æ®‹æŸæƒ…å†µï¼Œå»ºè®®ç ”ç©¶è€…æ ¹æ®åŸä»¶è¡¥å……ï¼Œå¦‚â€œå†Œå­æœ¬ï¼Œä¸¤å¼ å¯¹å¼€å¶ï¼Œé¦–å°¾ä¿±æ®‹â€ç­‰ã€‚ï¼‰")

    # å‚è€ƒæ–‡çŒ®ï¼ˆå¯é€‰ï¼‰ï¼šå°è¯•ä» candidate_insights / next_actions ä¸­æŠ½å–
    # refs: List[str] = []
    # for item in (answer.candidate_insights or []):
    #     if "ã€Š" in item and "ã€‹" in item:
    #         refs.append(item)
    # for item in (answer.next_actions or []):
    #     if "ã€Š" in item and "ã€‹" in item and item not in refs:
    #         refs.append(item)
    #
    # if refs:
    #     lines.append("å‚ï¼š")
    #     for idx, r in enumerate(refs, 1):
    #         lines.append(f"ï¼ˆ{idx}ï¼‰{r}")

    return "\n".join(lines)

def process_image(agent: CBETAAgent, image_path: Path, output_dir: Path, mirror_stdout: bool):
    print(f"\nğŸ“· å¤„ç†å›¾ç‰‡: {image_path.name}")
    
    # åˆ›å»ºä»¥å›¾ç‰‡åç§°å‘½åçš„å­æ–‡ä»¶å¤¹
    pic_output_dir = output_dir / image_path.stem
    pic_output_dir.mkdir(parents=True, exist_ok=True)
    
    log_path = pic_output_dir / f"{image_path.stem}_stream.jsonl"
    handler = build_stream_handler(log_path, mirror_stdout)
    try:
        result = agent.analyze_and_locate(image_path=str(image_path), stream_handler=handler)
    finally:
        if hasattr(handler, "close"):
            handler.close()  # type: ignore[attr-defined]

    if not result:
        print("âŒ æœ¬æ¬¡æœªè·å–åˆ°ç»“æ„åŒ–ç»“æœ")
        return

    json_path = pic_output_dir / f"{image_path.stem}_result.json"
    json_path.write_text(result.model_dump_json(indent=2, ensure_ascii=False), encoding="utf-8")
    print(f"ğŸ’¾ ç»“æ„åŒ–ç»“æœå·²ä¿å­˜: {json_path}")

    report_path = pic_output_dir / f"{image_path.stem}_report.txt"
    report_path.write_text(summarize_final_answer(result), encoding="utf-8")
    print(f"ğŸ“ æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

    # ç”Ÿæˆ"æ–‡çŒ®æ•´ç†è¯´æ˜"é™„å¸¦æ–‡æ¡£
    note_path = pic_output_dir / f"{image_path.stem}_note.txt"
    note_path.write_text(build_fragment_note(result, image_path.stem), encoding="utf-8")
    print(f"ğŸ“„ æ–‡çŒ®æ•´ç†è¯´æ˜å·²ä¿å­˜: {note_path}")


def main():
    args = parse_args()
    input_dir = Path(args.input)
    # ä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼
    output_dir = Path(args.output) if args.output else get_output_dir()

    images = list(iter_images(input_dir))
    if not images:
        print(f"âš ï¸ æœªåœ¨ {input_dir} æ‰¾åˆ°å›¾ç‰‡ï¼ˆæ”¯æŒ PNG/JPG/JPEGï¼‰ã€‚")
        return

    config = AgentConfig(verbose=not args.quiet)
    agent = CBETAAgent(config=config)

    for image_path in images:
        process_image(agent, image_path, output_dir, mirror_stdout=not args.quiet)

    print("\nâœ… å…¨éƒ¨å›¾ç‰‡å¤„ç†å®Œæˆã€‚")


if __name__ == "__main__":
    main()
